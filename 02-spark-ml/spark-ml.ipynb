{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e735f408-37a6-4e2b-be2b-c5b8513b08c3",
   "metadata": {},
   "source": [
    "# Spark ML: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe0e25-d616-4999-b9ce-2c7eb4e2b68f",
   "metadata": {},
   "source": [
    "Spark ML (Machine Learning) adalah pustaka untuk pembelajaran mesin di Apache Spark. Pustaka ini dirancang untuk memudahkan pengembangan, penerapan, dan pengelolaan algoritma pembelajaran mesin pada data besar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f8fbd-bf2a-4239-9c8c-6d92e006cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a9210-00cb-4426-a9bc-646c4b489c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext()\n",
    "spark = SparkSession.builder.appName(\"Python Spark ML basic example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09102c-6bb5-4414-9ebc-8550d3e56942",
   "metadata": {},
   "source": [
    "## Diagnostic Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea03724-952d-435f-bfb5-52e3b6108be5",
   "metadata": {},
   "source": [
    "Jenis diagnostic analytics berguna ketika lembaga, organisasi, atau perusahaan ingin mendapatkan wawasan mengenai masalah tertentu. Proses analisis dilakukan dengan melakukan pemulihan, pengembangan, dan penelusuran data. Data yang dimasukkan dalam analisis tentu saja lebih banyak dan bervariasi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ea0e4-808c-49a9-a37d-c3f6bf749c4a",
   "metadata": {},
   "source": [
    "* Tujuan: Menganalisis data untuk memahami penyebab di balik kejadian atau pola yang ditemukan.\n",
    "* Metode: Menggunakan teknik analisis lebih mendalam seperti analisis sebab-akibat dan regresi.\n",
    "\n",
    "Contoh: Menyelidiki penurunan performa penjualan dengan mencari tahu penyebabnya, seperti perubahan dalam strategi pemasaran atau faktor eksternal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97b245",
   "metadata": {},
   "source": [
    "Metode analisis yg digunakan:\n",
    "* Correlation: Mengukur kekuatan dan arah hubungan antara dua variabel dengan korelasi Pearson atau Spearman.\n",
    "* ChiSquareTest: Menguji independensi antara fitur dan label pada data kategorikal menggunakan uji chi-square.\n",
    "* Summarizer: Menghitung statistik deskriptif seperti rata-rata dan jumlah untuk fitur, baik dengan atau tanpa pembobotan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fcf39f-2d98-4ab5-a035-9efdf36e4521",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c523311",
   "metadata": {},
   "source": [
    "Mengukur kekuatan dan arah hubungan antara dua variabel.\n",
    "\n",
    "* Korelasi Pearson: Mengukur hubungan linier antara dua variabel. Nilai berkisar dari -1 hingga 1, di mana 1 menunjukkan hubungan linier positif sempurna, -1 menunjukkan hubungan linier negatif sempurna, dan 0 menunjukkan tidak ada hubungan linier.\n",
    "* Korelasi Spearman: Mengukur hubungan monotonic antara dua variabel. Ini tidak memerlukan hubungan linier dan cocok untuk data ordinal atau data yang tidak memenuhi asumsi linearitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca6468-9e6e-4947-8524-dbc13fd5b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3a8ca-e169-40ff-a1cf-967cfa18d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac1cba",
   "metadata": {},
   "source": [
    "* Vectors.sparse: Membuat vektor fitur yang jarang (sparse vector) dengan dimensi 4. Misalnya, Vectors.sparse(4, [(0, 1.0), (3, -2.0)]) berarti vektor dengan nilai 1.0 pada indeks 0 dan -2.0 pada indeks 3, sedangkan nilai lainnya adalah 0.\n",
    "* Vectors.dense: Membuat vektor fitur yang padat (dense vector). Misalnya, Vectors.dense([4.0, 5.0, 0.0, 3.0]) berarti vektor dengan nilai 4.0, 5.0, 0.0, dan 3.0 pada indeks yang sesuai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1e8d3",
   "metadata": {},
   "source": [
    "Pemilihan antara vektor jarang dan padat bergantung pada karakteristik data dan efisiensi yang diperlukan untuk pemrosesan dan penyimpanan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fef31e",
   "metadata": {},
   "source": [
    "* Vektor jarang adalah representasi dari vektor di mana sebagian besar elemen memiliki nilai nol. Hanya elemen-elemen yang tidak nol yang disimpan secara eksplisit.\n",
    "  * Vektor jarang hanya menyimpan nilai-nilai yang tidak nol dan posisi mereka. Ini menghemat memori, terutama untuk vektor dengan banyak elemen nol.\n",
    "  * Keuntungan: menggunakan lebih sedikit memori untuk vektor dengan banyak elemen nol, operasi matematis seperti perkalian matriks dapat lebih efisien pada vektor jarang.\n",
    "  * Digunakan bila data dengan banyak nilai nol, seperti representasi teks atau fitur dengan banyak kategori yang tidak aktif.\n",
    "* Vektor padat adalah representasi dari vektor di mana semua elemen, termasuk yang bernilai nol, disimpan secara eksplisit.\n",
    "  * Semua elemen dari vektor disimpan, tanpa memandang apakah mereka nol atau tidak. Ini memerlukan lebih banyak memori dibandingkan dengan vektor jarang jika banyak elemen adalah nol.\n",
    "  * Keuntungan: format yang lebih sederhana dan sering digunakan dalam algoritma yang memerlukan akses langsung ke setiap elemen, beberapa operasi matematis dan algoritma mungkin lebih mudah diterapkan pada vektor padat.\n",
    "  * Digunakan bila data yang tidak memiliki banyak elemen nol, atau ketika memori dan efisiensi penyimpanan tidak menjadi masalah besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4681a-66b2-4297-bd94-6768c38d1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1aee4",
   "metadata": {},
   "source": [
    "* spark.createDataFrame(data, [\"features\"]): Membuat DataFrame dari data yang berisi kolom \"features\". DataFrame ini akan memiliki satu kolom yang berisi vektor fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80923a4-fdae-47e0-805f-9a5657f11834",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc817b9e",
   "metadata": {},
   "source": [
    "* Correlation.corr(df, \"features\"): Menghitung matriks korelasi Pearson untuk kolom \"features\" dari DataFrame df. Fungsi ini mengembalikan matriks korelasi yang menunjukkan sejauh mana fitur dalam vektor fitur saling berkorelasi.\n",
    "* .head(): Mengambil baris pertama dari hasil matriks korelasi.\n",
    "* print(\"Pearson correlation matrix:\\n\" + str(r1[0])): Menampilkan matriks korelasi Pearson dalam format string. r1[0] berisi matriks korelasi dalam bentuk Row PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e746f5-f03f-4d26-9aa9-2978514d15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ffb35",
   "metadata": {},
   "source": [
    "* Correlation.corr(df, \"features\", \"spearman\"): Menghitung matriks korelasi Spearman untuk kolom \"features\" dari DataFrame df. Korelasi Spearman mengukur kekuatan dan arah hubungan monotonic antara dua variabel.\n",
    "  * df: DataFrame yang berisi kolom \"features\" dengan vektor fitur.\n",
    "  * \"features\": Nama kolom dalam DataFrame yang berisi vektor fitur yang akan dihitung korelasinya.\n",
    "  * \"spearman\": Jenis korelasi yang digunakan adalah korelasi Spearman. Korelasi Spearman berbeda dari korelasi Pearson karena mengukur hubungan monotonic dan tidak memerlukan asumsi linearitas.\n",
    "* print(\"Spearman correlation matrix:\\n\" + str(r2[0])): Menampilkan matriks korelasi Spearman yang telah dihitung. r2[0] berisi matriks korelasi dalam bentuk DenseMatrix yang diubah menjadi string untuk ditampilkan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65347130-f585-4e12-b65d-e92c27705870",
   "metadata": {},
   "source": [
    "### Chi Square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608de1c",
   "metadata": {},
   "source": [
    " Mengukur independensi antara fitur dan label dalam data kategorikal. Uji chi-square menguji apakah distribusi fitur berbeda secara signifikan berdasarkan label.\n",
    "\n",
    " * Uji Chi-Square: Berguna untuk data kategorikal untuk menentukan apakah ada asosiasi atau ketergantungan antara variabel kategorikal.\n",
    "* Statistik: Menghasilkan nilai p, derajat kebebasan, dan statistik chi-square yang digunakan untuk menentukan signifikansi hubungan antara fitur dan label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9ad7c",
   "metadata": {},
   "source": [
    "Contoh Penggunaan Uji Chi-Square\n",
    "\n",
    "* Menilai Asosiasi: Uji chi-square digunakan untuk menilai apakah ada asosiasi signifikan antara fitur kategorikal dan variabel target. Ini berguna dalam analisis data untuk memahami hubungan antara fitur dan target.\n",
    "* Penerapan dalam Model: Uji ini bisa digunakan dalam pemilihan fitur untuk model pembelajaran mesin untuk memilih fitur yang signifikan secara statistik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c75a7e-7d60-4bab-8c1c-66ee9ba05033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adf3d6-0640-43bc-bd74-985a8eda2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac3504",
   "metadata": {},
   "source": [
    "* Vectors.dense(): Membuat vektor fitur padat (dense vector) untuk setiap baris data. Misalnya, Vectors.dense(0.5, 10.0) adalah vektor dengan nilai 0.5 dan 10.0.\n",
    "* spark.createDataFrame(data, [\"label\", \"features\"]): Membuat DataFrame df dengan kolom \"label\" dan \"features\". Kolom \"label\" adalah variabel target, dan kolom \"features\" adalah fitur yang digunakan untuk uji chi-square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3e599",
   "metadata": {},
   "source": [
    "* ChiSquareTest.test(df, \"features\", \"label\"): Melakukan uji chi-square untuk mengukur ketergantungan antara fitur (\"features\") dan label (\"label\"). Uji ini mengevaluasi apakah distribusi fitur berbeda secara signifikan berdasarkan label.\n",
    "* r.pValues: Nilai p untuk uji chi-square, menunjukkan signifikansi statistik dari hubungan antara fitur dan label. Nilai p yang kecil menunjukkan bahwa ada hubungan signifikan antara fitur dan label.\n",
    "* r.degreesOfFreedom: Derajat kebebasan dari uji chi-square, menunjukkan jumlah parameter yang dapat bervariasi bebas dalam uji.\n",
    "* r.statistics: Statistik chi-square untuk setiap fitur, menunjukkan ukuran ketergantungan antara fitur dan label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a515a08-c7f4-4ab7-bed8-9a359176ca1f",
   "metadata": {},
   "source": [
    "### Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629dcbc7",
   "metadata": {},
   "source": [
    "Menghitung statistik deskriptif seperti rata-rata, jumlah, dan lainnya untuk fitur dalam data, baik dengan atau tanpa pembobotan.\n",
    "\n",
    "* Summarizer: Berguna untuk menghitung statistik seperti rata-rata (mean), jumlah (count), dan variansi (variance) untuk fitur vektor. Ini dapat dilakukan dengan mempertimbangkan pembobotan jika diperlukan.\n",
    "* Statistik: Memungkinkan perhitungan statistik deskriptif untuk setiap fitur vektor secara efisien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53cd90a-321a-4591-8794-cc026cbff00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2727d0",
   "metadata": {},
   "source": [
    "* sc.parallelize(): Mengonversi data menjadi RDD dan kemudian membuat DataFrame dari RDD tersebut.\n",
    "* Row(weight=..., features=...): Membuat baris data dengan kolom weight dan features. features adalah vektor fitur padat (dense vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2d0e9-3a46-4317-b644-f774966ad6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
    "\n",
    "# compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342e89c",
   "metadata": {},
   "source": [
    "* Summarizer.metrics(\"mean\", \"count\"): Membuat objek Summarizer yang akan menghitung rata-rata (\"mean\") dan jumlah (\"count\") untuk setiap vektor fitur.\n",
    "* df.select(summarizer.summary(df.features, df.weight)): Menghitung statistik untuk fitur dengan memperhitungkan bobot. summarizer.summary menghasilkan rata-rata dan jumlah berdasarkan bobot yang diberikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec33d77-c50b-4a46-9d31-a534fd30f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4df5d",
   "metadata": {},
   "source": [
    "* df.select(summarizer.summary(df.features)): Menghitung statistik untuk fitur tanpa mempertimbangkan bobot. Ini menghasilkan rata-rata dan jumlah berdasarkan nilai fitur langsung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ad7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics for single metric \"mean\" with weight\n",
    "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe10dc",
   "metadata": {},
   "source": [
    "* Summarizer.mean(df.features, df.weight): Menghitung rata-rata (mean) dari fitur dengan mempertimbangkan bobot. Hasilnya adalah rata-rata tertimbang dari fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7424e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics for single metric \"mean\" without weight\n",
    "df.select(Summarizer.mean(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6b01f",
   "metadata": {},
   "source": [
    "* Summarizer.mean(df.features): Menghitung rata-rata (mean) dari fitur tanpa mempertimbangkan bobot. Ini adalah rata-rata sederhana dari fitur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608d1b6",
   "metadata": {},
   "source": [
    "### Analisis Teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55fa07",
   "metadata": {},
   "source": [
    "Pipeline ini terdiri dari beberapa tahap, termasuk tokenisasi, vektorisasi hitung (Count Vectorization), dan perhitungan IDF (Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf9e02-e287-4449-ba98-adabe3a1d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL\")],\n",
    " [\"document\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0321419",
   "metadata": {},
   "source": [
    "* spark.createDataFrame([...], [\"document\", \"sentence\"]): Membuat DataFrame sentenceData dengan dua kolom: \"document\" dan \"sentence\". Kolom \"sentence\" berisi teks yang akan diproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab479e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "vectorizer  = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851e2bd",
   "metadata": {},
   "source": [
    "* Tokenisasi: Berguna untuk memecah teks menjadi kata-kata individu untuk analisis lebih lanjut.\n",
    "  * Tokenizer: Tokenizer membagi kalimat dalam kolom \"sentence\" menjadi daftar kata-kata individu. Outputnya adalah kolom \"words\".\n",
    "* CountVectorizer: Mengonversi teks menjadi representasi numerik yang dapat digunakan untuk pembelajaran mesin. \n",
    "  * CountVectorizer: Mengubah daftar kata-kata (\"words\") menjadi vektor fitur hitung (count vector). Outputnya adalah kolom \"rawFeatures\", di mana setiap kata dalam dokumen dipetakan ke vektor hitung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd5407",
   "metadata": {},
   "source": [
    "* IDF: Menyaring fitur penting dari fitur yang jarang muncul, meningkatkan kualitas model pembelajaran mesin dengan memberikan bobot yang lebih besar pada kata-kata yang lebih informatif.\n",
    "  * Fungsi IDF: Menghitung nilai IDF dari vektor hitung. IDF digunakan untuk mengukur pentingnya kata dalam korpus dokumen secara keseluruhan. Outputnya adalah kolom \"features\" yang berisi vektor fitur yang telah ditimbang dengan IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cedb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "model = pipeline.fit(sentenceData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02108293",
   "metadata": {},
   "source": [
    "* Pipeline(stages=[tokenizer, vectorizer, idf]): Membuat pipeline yang menggabungkan semua tahapan (tokenisasi, vektorisasi hitung, dan IDF) dalam satu alur kerja.\n",
    "* pipeline.fit(sentenceData): Melatih pipeline dengan data sentenceData. Ini mencakup proses tokenisasi, pembuatan vektor hitung, dan perhitungan IDF, menghasilkan model yang sudah terlatih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a30a9-52eb-4b75-b005-3ae486a0fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_counts = model.transform(sentenceData)\\\n",
    "                    .select('rawFeatures').rdd\\\n",
    "                    .map(lambda row: row['rawFeatures'].toArray())\\\n",
    "                    .reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65505f1b",
   "metadata": {},
   "source": [
    "* Transformasi: Menggunakan model pipeline untuk menghasilkan vektor hitung dari data kalimat, kemudian menghitung total frekuensi setiap kata di seluruh dokumen.\n",
    "  * model.transform(sentenceData): Menerapkan model pipeline yang telah dilatih pada sentenceData, menghasilkan DataFrame dengan kolom baru \"rawFeatures\" yang berisi vektor hitung fitur.\n",
    "  * .select('rawFeatures'): Memilih kolom \"rawFeatures\" dari hasil transformasi.\n",
    "  * .rdd: Mengonversi DataFrame menjadi RDD (Resilient Distributed Dataset).\n",
    "  * .map(lambda row: row['rawFeatures'].toArray()): Mengonversi vektor fitur dari SparseVector menjadi array NumPy. Fungsi lambda ini diterapkan pada setiap baris.\n",
    "  * .reduce(lambda x, y: [x[i] + y[i] for i in range(len(y))]): Menjumlahkan array vektor hitung untuk setiap kata, menghasilkan total frekuensi kata di seluruh dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList = model.stages[1].vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067afa3f",
   "metadata": {},
   "source": [
    "* model.stages[1].vocabulary: Mengambil daftar kosakata (vocabulary) dari CountVectorizer (stages[1] dalam pipeline), yang merupakan urutan kata-kata yang diindeks berdasarkan urutan kemunculannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e56d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733956d",
   "metadata": {},
   "source": [
    "* d = {'vocabList':vocabList, 'counts':total_counts}: Membuat dictionary d dengan kunci 'vocabList' dan 'counts', di mana 'vocabList' adalah daftar kosakata dan 'counts' adalah frekuensi total kata.\n",
    "* np.array(list(d.values())).T.tolist(): Mengonversi dictionary ke dalam format array NumPy, mentransposkan array, dan kemudian mengonversi ke daftar Python (list) yang dapat digunakan untuk membuat DataFrame.\n",
    "* spark.createDataFrame(..., list(d.keys())): Membuat DataFrame Spark dari data yang telah disiapkan, menggunakan kunci dictionary sebagai nama kolom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d0260-61ed-4ba7-87d5-da673eb3c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(sentenceData).show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319b200-18c2-4c23-8199-618e54c50b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ad8d6",
   "metadata": {},
   "source": [
    "* model.transform(sentenceData).show(truncate=True): Menampilkan DataFrame hasil transformasi dari sentenceData setelah pipeline diterapkan. truncate=True memastikan kolom yang panjang dipotong untuk tampilan yang lebih ringkas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6069c-232a-4c04-8a08-610b17f91ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return udf(termsIdx2Term, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80eca9b",
   "metadata": {},
   "source": [
    "Membuat fungsi mengonversi indeks kata menjadi kata asli:\n",
    "* termsIdx2Term(vocabulary): Ini adalah fungsi yang menerima vocabulary (daftar kosakata) dan mengembalikan fungsi UDF (User Defined Function) PySpark.\n",
    "* termsIdx2Term(termIndices): Ini adalah fungsi internal yang mengonversi indeks kata menjadi kata itu sendiri. termIndices adalah daftar indeks yang akan dikonversi.\n",
    "* return [vocabulary[int(index)] for index in termIndices]: Mengonversi setiap indeks dalam termIndices menjadi kata menggunakan vocabulary dan mengembalikan daftar kata.\n",
    "* return udf(termsIdx2Term, ArrayType(StringType())): Membuat UDF PySpark yang menggunakan fungsi termsIdx2Term. UDF ini akan digunakan untuk mengonversi vektor fitur menjadi daftar kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerModel = model.stages[1]\n",
    "vocabList = vectorizerModel.vocabulary\n",
    "vocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97093561",
   "metadata": {},
   "source": [
    "* vectorizerModel = model.stages[1]: Mengambil model CountVectorizer dari pipeline yang telah dilatih.\n",
    "* vocabList = vectorizerModel.vocabulary: Mengambil daftar kosakata dari CountVectorizer. Daftar ini berisi kata-kata yang digunakan dalam model vektorisasi hitung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b756a9e-b7b7-4dc3-b49d-de1fee286023",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawFeatures = model.transform(sentenceData).select('rawFeatures')\n",
    "rawFeatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a042ca1",
   "metadata": {},
   "source": [
    "* model.transform(sentenceData): Menerapkan model pipeline pada DataFrame sentenceData.\n",
    "* .select('rawFeatures'): Memilih kolom rawFeatures yang berisi vektor hitung dari CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59caca0f-84fe-482a-81c3-bb9a202d5e55",
   "metadata": {},
   "source": [
    "## Predictive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba94e74",
   "metadata": {},
   "source": [
    "Predictive Analytics adalah metode analitik yang menggunakan data, statistik, dan teknik machine learning untuk membuat prediksi tentang kejadian atau hasil masa depan. Ini melibatkan penggunaan data historis untuk membangun model yang dapat memproyeksikan tren, pola, atau nilai di masa depan. Tujuannya adalah untuk memberikan wawasan yang bermanfaat dan mendukung pengambilan keputusan berdasarkan prediksi yang dihasilkan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee4961",
   "metadata": {},
   "source": [
    "Pada kali ini akan dicoba beberapa metode umum:\n",
    "* Regresi: Memodelkan hubungan antara fitur dan variabel target untuk prediksi nilai kontinu atau probabilitas.\n",
    "* Decision Tree: Menggunakan struktur pohon keputusan untuk memprediksi kelas atau nilai berdasarkan fitur.\n",
    "* K-Means: Mengelompokkan data ke dalam klaster berdasarkan kesamaan fitur untuk segmentasi data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb2f7c-44fc-4f7c-ac54-cf8e15bbdffe",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b909f",
   "metadata": {},
   "source": [
    "Regresi adalah teknik statistik yang digunakan untuk mengukur hubungan antara variabel independen (fitur) dan variabel dependen (target). Ini bertujuan untuk memprediksi nilai kontinu dari variabel dependen berdasarkan variabel independen.\n",
    "\n",
    "Metode:\n",
    "* Regresi Linier: Memprediksi nilai target sebagai kombinasi linier dari fitur-fitur. Contoh: Memprediksi harga rumah berdasarkan ukuran dan lokasi.\n",
    "* Regresi Logistik: Digunakan untuk klasifikasi biner, memprediksi probabilitas dari dua kelas yang berbeda. Contoh: Memprediksi apakah email adalah spam atau bukan spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059298b-06f0-45bf-bd97-9ab29127cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84741bf0-94c1-474f-aebe-4c6dff77699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df = sqlContext.read.load('housing.csv',sep=\" \", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "house_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedbba4",
   "metadata": {},
   "source": [
    "* sqlContext.read.load(...): Membaca data dari file CSV housing.csv dengan pemisah spasi (sep=\" \"), menggunakan format CSV, dan secara otomatis menyimpulkan tipe data (inferSchema=\"true\"). Baris pertama dianggap sebagai header (header=\"true\").\n",
    "* house_df.take(5): Mengambil dan menampilkan 5 baris pertama dari DataFrame untuk melihat data yang telah dimuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc7b57-9a1c-430f-afbd-d15ed59b817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df.cache()\n",
    "house_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf0fff",
   "metadata": {},
   "source": [
    "* house_df.cache(): Menyimpan DataFrame di memori untuk meningkatkan performa jika DataFrame digunakan berkali-kali.\n",
    "* house_df.printSchema(): Menampilkan skema DataFrame, yaitu struktur kolom dan tipe datanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e98fd-e670-49b3-abb0-498152a949af",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4c7b2",
   "metadata": {},
   "source": [
    "* house_df.describe(): Menghitung statistik deskriptif seperti mean, min, max untuk setiap kolom numerik.\n",
    "* .toPandas().transpose(): Mengonversi hasil statistik deskriptif ke DataFrame Pandas dan mentranspose-nya agar lebih mudah dibaca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f525c0d-f241-448f-b3b1-f78ad7f26121",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482088c-0e64-4639-9c1c-9371f3a42391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "numeric_features = [t[0] for t in house_df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = house_df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "type(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42682642",
   "metadata": {},
   "source": [
    "* numeric_features: Menyaring kolom-kolom yang bertipe int atau double dari DataFrame.\n",
    "* sample(False, 0.8): Mengambil sampel 80% dari data secara acak tanpa pengulangan.\n",
    "* .toPandas(): Mengonversi hasil sampling ke DataFrame Pandas.\n",
    "type(sampled_data): Mengecek tipe data dari sampled_data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f17a1f-7dba-4ec5-b21f-463193066e09",
   "metadata": {},
   "source": [
    "setelah menjadi datarame pandas, dapat dilakukan proses data analitik seperti pada python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67660d-c3a7-46a8-805c-a43471284120",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f617a",
   "metadata": {},
   "source": [
    "* house_df.columns: Menampilkan nama-nama kolom dalam DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c2827-d019-4b8a-86e2-504b829f0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ad6f0",
   "metadata": {},
   "source": [
    "* house_df.show(3): Menampilkan 3 baris pertama dari DataFrame untuk pemeriksaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996aed60-d441-493f-890f-c7cbbb6d32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "inputCols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'RM', 'AGE',\n",
    "             'DIS', 'RAD', 'TAX', 'PT', 'B', 'LSTAT', 'MV']\n",
    "outputCol = \"features\"\n",
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "df = df_va.transform(house_df)\n",
    "df = df.select(['nox','features'])\n",
    "newcolumns = ['label','features']\n",
    "df.toDF(*newcolumns).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7f9c2",
   "metadata": {},
   "source": [
    "* VectorAssembler: Menggabungkan kolom fitur menjadi satu kolom fitur vektor (features).\n",
    "* inputCols: Daftar kolom fitur yang akan digabungkan.\n",
    "* outputCol: Nama kolom hasil gabungan fitur.\n",
    "* df_va.transform(house_df): Menerapkan VectorAssembler untuk DataFrame house_df dan menghasilkan DataFrame baru dengan kolom features.\n",
    "* df.select(['nox', 'features']): Memilih kolom nox sebagai label dan features sebagai kolom fitur.\n",
    "* df.toDF(*newcolumns): Mengganti nama kolom menjadi label dan features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63383b-a97b-430a-bf7f-bdd685c2b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membagi data\n",
    "(trainingData, testData) = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f1a73",
   "metadata": {},
   "source": [
    "* randomSplit([0.7, 0.3]): Membagi data menjadi dua subset: 70% untuk pelatihan (trainingData) dan 30% untuk pengujian (testData)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4ce82-52be-48ed-bd8f-384f7029c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecc48e-b27f-48ee-90fa-e8b95beb678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbf753-ab39-4360-ba4d-c8c7b01b5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Melatih model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"nox\")\n",
    "model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86bee2",
   "metadata": {},
   "source": [
    "* LinearRegression: Membuat objek model regresi linier dengan kolom fitur (featuresCol) dan kolom label (labelCol).\n",
    "* lr.fit(trainingData): Melatih model regresi linier dengan data pelatihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfe350-423c-4017-bf3d-8a253d776abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat koefisien model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "\n",
    "# Melihat intercept model\n",
    "print(\"Intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e723e47",
   "metadata": {},
   "source": [
    "* model.coefficients: Menampilkan koefisien model.\n",
    "* model.intercept: Menampilkan intercept model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c264f-26b0-47a0-a344-6495a31876fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mendapatkan ringkasan model\n",
    "trainingSummary = model.summary\n",
    "trainingSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ab9dc",
   "metadata": {},
   "source": [
    "* model.summary: Mengambil ringkasan model yang berisi metrik evaluasi seperti RMSE, R2, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddeb91-6e43-4b3a-9402-71533f021a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memprediksi\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f8b13",
   "metadata": {},
   "source": [
    "* model.transform(testData): Menerapkan model yang telah dilatih untuk membuat prediksi pada data pengujian (testData)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b1aa6-f2a5-41eb-b2b3-49e87afe6965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7aa8a-018e-45b3-bb42-62aec7ac901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Evaluasi\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\",labelCol=\"nox\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab8e40",
   "metadata": {},
   "source": [
    "* model.transform(testData): Menerapkan model yang telah dilatih untuk membuat prediksi pada data pengujian (testData).\n",
    "* predictions.show(): Menampilkan hasil prediksi.\n",
    "* RegressionEvaluator: Mengukur kinerja model regresi.\n",
    "* evaluator.evaluate(predictions): Mengevaluasi hasil prediksi menggunakan metrik seperti RMSE (nilai default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfd04b-7603-42c4-9eef-2e7ec6cf9764",
   "metadata": {},
   "source": [
    "### Decision Tree Classiication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b0a3a",
   "metadata": {},
   "source": [
    "Decision Tree adalah model yang membagi data menjadi subset berdasarkan fitur dengan membuat keputusan berbasis percabangan. Setiap node internal mewakili tes pada fitur, dan setiap cabang mewakili hasil tes. Daun akhir mewakili hasil prediksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184bf08",
   "metadata": {},
   "source": [
    "Penerapan:\n",
    "* Decision Tree untuk Klasifikasi: Memprediksi kelas dari data input berdasarkan fitur-fitur. Contoh: Memprediksi apakah seorang pelanggan akan membeli produk atau tidak.\n",
    "* Decision Tree untuk Regresi: Memprediksi nilai kontinu dengan membagi data pada nilai fitur. Contoh: Memprediksi harga properti berdasarkan fitur-fitur seperti ukuran dan lokasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc9018-2c20-4b30-87cd-3ee0864156a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "data = df.toDF(*newcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666ee71-b945-42ea-bc11-207066f00ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a72710",
   "metadata": {},
   "source": [
    "* df.toDF(*newcolumns): Mengonversi DataFrame df dengan kolom yang dinamai ulang menjadi DataFrame Spark yang siap digunakan untuk model pembelajaran mesin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea26f5c-ab74-4291-bbcc-ec03c09fc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c50529",
   "metadata": {},
   "source": [
    "* StringIndexer: Mengonversi label kategorikal menjadi label numerik yang dapat diproses oleh model pembelajaran mesin.\n",
    "* inputCol=\"label\": Kolom asli yang berisi label kategorikal.\n",
    "* outputCol=\"indexedLabel\": Nama kolom hasil yang berisi label numerik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8216dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8074aff",
   "metadata": {},
   "source": [
    "\n",
    "* VectorIndexer: Mengidentifikasi fitur kategorikal dalam kolom fitur dan mengonversinya menjadi format numerik. Fitur dengan lebih dari 4 kategori dianggap sebagai fitur kontinu.\n",
    "* inputCol=\"features\": Kolom yang berisi fitur.\n",
    "* outputCol=\"indexedFeatures\": Nama kolom hasil yang berisi fitur yang diindeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7add727-f9bf-4c9b-a50e-144377d49da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7fa0f",
   "metadata": {},
   "source": [
    "* randomSplit([0.7, 0.3]): Membagi data menjadi dua subset: 70% untuk pelatihan (trainingData) dan 30% untuk pengujian (testData)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5079b-d099-41e7-8f22-dd59bdc98688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71ad47-e9a5-424b-b1da-ad2a6b692a90",
   "metadata": {},
   "source": [
    "* DecisionTreeClassifier: Membuat model klasifikasi menggunakan pohon keputusan dengan kolom label yang diindeks dan fitur yang diindeks.\n",
    "* Pipeline: Menggabungkan beberapa tahap pemrosesan dan pelatihan model ke dalam satu objek. Pipeline ini termasuk indeksasi label, indeksasi fitur, dan pelatihan model pohon keputusan.\n",
    "* pipeline.fit(trainingData): Melatih model dengan data pelatihan, yang juga menerapkan langkah-langkah indeksasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf37933-b7e4-45db-ad80-7f4443bf7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a570b9",
   "metadata": {},
   "source": [
    "* model.transform(testData): Menggunakan model yang telah dilatih untuk memprediksi label pada data pengujian.\n",
    "* predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5): Menampilkan beberapa contoh hasil prediksi bersama dengan label yang sebenarnya dan fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47a0f1-4917-4430-94fe-3bd1c9cc1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Acccuracy = %g \" % accuracy)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cdf01",
   "metadata": {},
   "source": [
    "* MulticlassClassificationEvaluator: Evaluasi model klasifikasi multikelas dengan metrik seperti akurasi.\n",
    "* evaluator.evaluate(predictions): Menghitung akurasi model berdasarkan prediksi dan label yang sebenarnya.\n",
    "* print(\"Accuracy = %g \" % accuracy): Menampilkan akurasi model.\n",
    "* print(\"Test Error = %g \" % (1.0 - accuracy)): Menampilkan tingkat kesalahan model (1 - akurasi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a72596-95df-41aa-baf8-c9371a045076",
   "metadata": {},
   "outputs": [],
   "source": [
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8f182",
   "metadata": {},
   "source": [
    "* model.stages[2]: Mengambil model pohon keputusan dari tahap ketiga pipeline (karena labelIndexer dan featureIndexer adalah tahap pertama dan kedua).\n",
    "* print(treeModel): Menampilkan ringkasan model pohon keputusan, termasuk struktur pohon dan informasi tentang aturan klasifikasi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2a34d-ca9e-4a62-b3b4-8360ba7e95a8",
   "metadata": {},
   "source": [
    "### K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d92b69",
   "metadata": {},
   "source": [
    "K-Means adalah algoritma clustering yang digunakan untuk mengelompokkan data ke dalam sejumlah klaster yang telah ditentukan. Algoritma ini bekerja dengan membagi data ke dalam klaster yang meminimalkan jarak intra-klaster (jarak antara data dalam klaster) dan memaksimalkan jarak antar-klaster (jarak antara klaster yang berbeda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87408bd8-fe89-4e2a-a41d-7cfb554ef0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"sample_kmeans_data.txt\")\n",
    "dataset.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4b774-f4fe-44b3-8f6b-21173cc53bc0",
   "metadata": {},
   "source": [
    "* spark.read.format(\"libsvm\").load(\"sample_kmeans_data.txt\"): Membaca data dari file sample_kmeans_data.txt dalam format LIBSVM, yang umumnya digunakan untuk data fitur yang sudah dinormalisasi dan dikodekan. Format LIBSVM adalah format file yang sering digunakan untuk data pembelajaran mesin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f3720-b6a5-447f-981a-b5bf1907fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59f712",
   "metadata": {},
   "source": [
    "* KMeans().setK(2).setSeed(1): Membuat objek KMeans dengan K=2, artinya algoritma akan mencoba membagi data menjadi 2 klaster. setSeed(1) digunakan untuk memastikan reprodusibilitas hasil.\n",
    "* kmeans.fit(dataset): Melatih model KMeans menggunakan dataset yang telah dimuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844eee6-b660-4893-8639-312fc33d0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9d22b",
   "metadata": {},
   "source": [
    "* model.transform(dataset): Menggunakan model yang telah dilatih untuk memprediksi klaster untuk setiap baris data dalam dataset. Hasilnya adalah DataFrame yang mencakup kolom baru prediction yang menunjukkan klaster mana yang ditetapkan untuk setiap data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c0bcd-758d-4860-881b-673cb820ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9cb316",
   "metadata": {},
   "source": [
    "* ClusteringEvaluator(): Membuat evaluator untuk klasterisasi yang dapat digunakan untuk menilai kualitas klaster.\n",
    "* evaluator.evaluate(predictions): Menghitung skor Silhouette berdasarkan hasil prediksi klaster. Skor Silhouette adalah metrik untuk menilai seberapa baik data diklasifikasikan ke dalam klaster. Nilai mendekati 1 menunjukkan klaster yang baik, sedangkan nilai mendekati -1 menunjukkan klaster yang buruk.\n",
    "* print(\"Silhouette with squared euclidean distance = \" + str(silhouette)): Menampilkan skor Silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747302f-71a5-4b66-85d4-c1c626dbe68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f9773",
   "metadata": {},
   "source": [
    "* model.clusterCenters(): Mengambil pusat klaster (centroid) yang ditemukan oleh model KMeans.\n",
    "* print(\"Cluster Centers: \"): Menampilkan pusat-pusat klaster.\n",
    "* for center in centers: print(center): Menampilkan masing-masing pusat klaster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354b30d",
   "metadata": {},
   "source": [
    "## Terima kasih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdde5d9-f375-4641-9488-93c0ed184cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
