{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e735f408-37a6-4e2b-be2b-c5b8513b08c3",
   "metadata": {},
   "source": [
    "# Spark ML: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe0e25-d616-4999-b9ce-2c7eb4e2b68f",
   "metadata": {},
   "source": [
    "Spark ML (Machine Learning) adalah pustaka untuk pembelajaran mesin di Apache Spark. Pustaka ini dirancang untuk memudahkan pengembangan, penerapan, dan pengelolaan algoritma pembelajaran mesin pada data besar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182f8fbd-bf2a-4239-9c8c-6d92e006cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9a9210-00cb-4426-a9bc-646c4b489c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext()\n",
    "spark = SparkSession.builder.appName(\"Python Spark ML basic example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09102c-6bb5-4414-9ebc-8550d3e56942",
   "metadata": {},
   "source": [
    "## Diagnostic Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea03724-952d-435f-bfb5-52e3b6108be5",
   "metadata": {},
   "source": [
    "Jenis diagnostic analytics berguna ketika lembaga, organisasi, atau perusahaan ingin mendapatkan wawasan mengenai masalah tertentu. Proses analisis dilakukan dengan melakukan pemulihan, pengembangan, dan penelusuran data. Data yang dimasukkan dalam analisis tentu saja lebih banyak dan bervariasi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ea0e4-808c-49a9-a37d-c3f6bf749c4a",
   "metadata": {},
   "source": [
    "* Tujuan: Menganalisis data untuk memahami penyebab di balik kejadian atau pola yang ditemukan.\n",
    "* Metode: Menggunakan teknik analisis lebih mendalam seperti analisis sebab-akibat dan regresi.\n",
    "\n",
    "Contoh: Menyelidiki penurunan performa penjualan dengan mencari tahu penyebabnya, seperti perubahan dalam strategi pemasaran atau faktor eksternal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fcf39f-2d98-4ab5-a035-9efdf36e4521",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ca6468-9e6e-4947-8524-dbc13fd5b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b3a8ca-e169-40ff-a1cf-967cfa18d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SparseVector(4, {0: 1.0, 3: -2.0}),),\n",
       " (DenseVector([4.0, 5.0, 0.0, 3.0]),),\n",
       " (DenseVector([6.0, 7.0, 0.0, 8.0]),),\n",
       " (SparseVector(4, {0: 9.0, 3: 1.0}),)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be4681a-66b2-4297-bd94-6768c38d1311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80923a4-fdae-47e0-805f-9a5657f11834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e746f5-f03f-4d26-9aa9-2978514d15f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65347130-f585-4e12-b65d-e92c27705870",
   "metadata": {},
   "source": [
    "### Chi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c75a7e-7d60-4bab-8c1c-66ee9ba05033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a515a08-c7f4-4ab7-bed8-9a359176ca1f",
   "metadata": {},
   "source": [
    "### Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53cd90a-321a-4591-8794-cc026cbff00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|weight|     features|\n",
      "+------+-------------+\n",
      "|   1.0|[1.0,1.0,1.0]|\n",
      "|   0.0|[1.0,2.0,3.0]|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa2d0e9-3a46-4317-b644-f774966ad6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec33d77-c50b-4a46-9d31-a534fd30f154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|aggregate_metrics(features, weight)|\n",
      "+-----------------------------------+\n",
      "|{[1.0,1.0,1.0], 1}                 |\n",
      "+-----------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|aggregate_metrics(features, 1.0)|\n",
      "+--------------------------------+\n",
      "|{[1.0,1.5,2.0], 2}              |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.0,1.0] |\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.5,2.0] |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "\n",
    "# compute statistics for single metric \"mean\" with weight\n",
    "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for single metric \"mean\" without weight\n",
    "df.select(Summarizer.mean(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92cf9e02-e287-4449-ba98-adabe3a1d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL\")],\n",
    " [\"document\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "vectorizer  = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "model = pipeline.fit(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4a30a9-52eb-4b75-b005-3ae486a0fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|vocabList|counts|\n",
      "+---------+------+\n",
      "|   python|   3.0|\n",
      "|    spark|   2.0|\n",
      "|      sql|   1.0|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_counts = model.transform(sentenceData)\\\n",
    "                    .select('rawFeatures').rdd\\\n",
    "                    .map(lambda row: row['rawFeatures'].toArray())\\\n",
    "                    .reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "\n",
    "vocabList = model.stages[1].vocabulary\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20d0260-61ed-4ba7-87d5-da673eb3c941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "|document|            sentence|               words|        rawFeatures|            features|\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "|       0|Python python Spa...|[python, python, ...|(3,[0,1],[2.0,2.0])|(3,[0,1],[0.0,0.8...|\n",
      "|       1|          Python SQL|       [python, sql]|(3,[0,2],[1.0,1.0])|(3,[0,2],[0.0,0.4...|\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(sentenceData).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f6069c-232a-4c04-8a08-610b17f91ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'spark', 'sql']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return udf(termsIdx2Term, ArrayType(StringType()))\n",
    "\n",
    "vectorizerModel = model.stages[1]\n",
    "vocabList = vectorizerModel.vocabulary\n",
    "vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b756a9e-b7b7-4dc3-b49d-de1fee286023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        rawFeatures|\n",
      "+-------------------+\n",
      "|(3,[0,1],[2.0,2.0])|\n",
      "|(3,[0,2],[1.0,1.0])|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawFeatures = model.transform(sentenceData).select('rawFeatures')\n",
    "rawFeatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59caca0f-84fe-482a-81c3-bb9a202d5e55",
   "metadata": {},
   "source": [
    "## Predictive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb2f7c-44fc-4f7c-ac54-cf8e15bbdffe",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8059298b-06f0-45bf-bd97-9ab29127cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84741bf0-94c1-474f-aebe-4c6dff77699a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.00632, ZN=18.0, INDUS=2.31, CHAS=0, nox=0.538, RM=6.575, AGE=65.2, DIS=4.09, RAD=1, TAX=296.0, PT=15.3, B=396.9, LSTAT=4.98, MV=24.0),\n",
       " Row(CRIM=0.02731, ZN=0.0, INDUS=7.07, CHAS=0, nox=0.469, RM=6.421, AGE=78.9, DIS=4.9671, RAD=2, TAX=242.0, PT=17.8, B=396.9, LSTAT=9.14, MV=21.6),\n",
       " Row(CRIM=0.02729, ZN=0.0, INDUS=7.07, CHAS=0, nox=0.469, RM=7.185, AGE=61.1, DIS=4.9671, RAD=2, TAX=242.0, PT=17.8, B=392.83, LSTAT=4.03, MV=34.7),\n",
       " Row(CRIM=0.03237, ZN=0.0, INDUS=2.18, CHAS=0, nox=0.458, RM=6.998, AGE=45.8, DIS=6.0622, RAD=3, TAX=222.0, PT=18.7, B=394.63, LSTAT=2.94, MV=33.4),\n",
       " Row(CRIM=0.06905, ZN=0.0, INDUS=2.18, CHAS=0, nox=0.458, RM=7.147, AGE=54.2, DIS=6.0622, RAD=3, TAX=222.0, PT=18.7, B=396.9, LSTAT=5.33, MV=36.2)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df = sqlContext.read.load('housing.csv',sep=\" \", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "house_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cadc7b57-9a1c-430f-afbd-d15ed59b817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CRIM: double (nullable = true)\n",
      " |-- ZN: double (nullable = true)\n",
      " |-- INDUS: double (nullable = true)\n",
      " |-- CHAS: integer (nullable = true)\n",
      " |-- nox: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- DIS: double (nullable = true)\n",
      " |-- RAD: integer (nullable = true)\n",
      " |-- TAX: double (nullable = true)\n",
      " |-- PT: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- LSTAT: double (nullable = true)\n",
      " |-- MV: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_df.cache()\n",
    "house_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c7e98fd-e670-49b3-abb0-498152a949af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>506</td>\n",
       "      <td>3.6135235573122535</td>\n",
       "      <td>8.601545105332491</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>88.9762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>506</td>\n",
       "      <td>11.363636363636363</td>\n",
       "      <td>23.32245299451514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>506</td>\n",
       "      <td>11.136778656126504</td>\n",
       "      <td>6.860352940897589</td>\n",
       "      <td>0.46</td>\n",
       "      <td>27.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>506</td>\n",
       "      <td>0.0691699604743083</td>\n",
       "      <td>0.2539940413404101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>506</td>\n",
       "      <td>0.5546950592885372</td>\n",
       "      <td>0.11587767566755584</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>506</td>\n",
       "      <td>6.284634387351787</td>\n",
       "      <td>0.7026171434153232</td>\n",
       "      <td>3.561</td>\n",
       "      <td>8.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>506</td>\n",
       "      <td>68.57490118577078</td>\n",
       "      <td>28.148861406903595</td>\n",
       "      <td>2.9</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>506</td>\n",
       "      <td>3.795042687747034</td>\n",
       "      <td>2.10571012662761</td>\n",
       "      <td>1.1296</td>\n",
       "      <td>12.1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>506</td>\n",
       "      <td>9.549407114624506</td>\n",
       "      <td>8.707259384239366</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>506</td>\n",
       "      <td>408.2371541501976</td>\n",
       "      <td>168.53711605495903</td>\n",
       "      <td>187.0</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT</th>\n",
       "      <td>506</td>\n",
       "      <td>18.455533596837967</td>\n",
       "      <td>2.1649455237144455</td>\n",
       "      <td>12.6</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>506</td>\n",
       "      <td>356.67403162055257</td>\n",
       "      <td>91.29486438415782</td>\n",
       "      <td>0.32</td>\n",
       "      <td>396.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>506</td>\n",
       "      <td>12.653063241106723</td>\n",
       "      <td>7.141061511348571</td>\n",
       "      <td>1.73</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MV</th>\n",
       "      <td>506</td>\n",
       "      <td>22.532806324110698</td>\n",
       "      <td>9.197104087379815</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                   1                    2        3        4\n",
       "summary  count                mean               stddev      min      max\n",
       "CRIM       506  3.6135235573122535    8.601545105332491  0.00632  88.9762\n",
       "ZN         506  11.363636363636363    23.32245299451514      0.0    100.0\n",
       "INDUS      506  11.136778656126504    6.860352940897589     0.46    27.74\n",
       "CHAS       506  0.0691699604743083   0.2539940413404101        0        1\n",
       "nox        506  0.5546950592885372  0.11587767566755584    0.385    0.871\n",
       "RM         506   6.284634387351787   0.7026171434153232    3.561     8.78\n",
       "AGE        506   68.57490118577078   28.148861406903595      2.9    100.0\n",
       "DIS        506   3.795042687747034     2.10571012662761   1.1296  12.1265\n",
       "RAD        506   9.549407114624506    8.707259384239366        1       24\n",
       "TAX        506   408.2371541501976   168.53711605495903    187.0    711.0\n",
       "PT         506  18.455533596837967   2.1649455237144455     12.6     22.0\n",
       "B          506  356.67403162055257    91.29486438415782     0.32    396.9\n",
       "LSTAT      506  12.653063241106723    7.141061511348571     1.73    37.97\n",
       "MV         506  22.532806324110698    9.197104087379815      5.0     50.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f525c0d-f241-448f-b3b1-f78ad7f26121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3482088c-0e64-4639-9c1c-9371f3a42391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "numeric_features = [t[0] for t in house_df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = house_df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "type(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f17a1f-7dba-4ec5-b21f-463193066e09",
   "metadata": {},
   "source": [
    "setelah menjadi panda, dapat dilakukan prosesdata analitik seperti pada python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f67660d-c3a7-46a8-805c-a43471284120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CRIM',\n",
       " 'ZN',\n",
       " 'INDUS',\n",
       " 'CHAS',\n",
       " 'nox',\n",
       " 'RM',\n",
       " 'AGE',\n",
       " 'DIS',\n",
       " 'RAD',\n",
       " 'TAX',\n",
       " 'PT',\n",
       " 'B',\n",
       " 'LSTAT',\n",
       " 'MV']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "854c2827-d019-4b8a-86e2-504b829f0251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+-----+----+------+-----+----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  nox|   RM| AGE|   DIS|RAD|  TAX|  PT|     B|LSTAT|  MV|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+-----+----+------+-----+----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296.0|15.3| 396.9| 4.98|24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242.0|17.8| 396.9| 9.14|21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242.0|17.8|392.83| 4.03|34.7|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+-----+----+------+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "996aed60-d441-493f-890f-c7cbbb6d32f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------+\n",
      "|label|features                                                                   |\n",
      "+-----+---------------------------------------------------------------------------+\n",
      "|0.538|[0.00632,18.0,2.31,0.0,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98,24.0]     |\n",
      "|0.469|[0.02731,0.0,7.07,0.0,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14,21.6]    |\n",
      "|0.469|[0.02729,0.0,7.07,0.0,7.185,61.1,4.9671,2.0,242.0,17.8,392.83,4.03,34.7]   |\n",
      "|0.458|[0.03237,0.0,2.18,0.0,6.998,45.8,6.0622,3.0,222.0,18.7,394.63,2.94,33.4]   |\n",
      "|0.458|[0.06905,0.0,2.18,0.0,7.147,54.2,6.0622,3.0,222.0,18.7,396.9,5.33,36.2]    |\n",
      "|0.458|[0.02985,0.0,2.18,0.0,6.43,58.7,6.0622,3.0,222.0,18.7,394.12,5.21,28.7]    |\n",
      "|0.524|[0.08829,12.5,7.87,0.0,6.012,66.6,5.5605,5.0,311.0,15.2,395.6,12.43,22.9]  |\n",
      "|0.524|[0.14455,12.5,7.87,0.0,6.172,96.1,5.9505,5.0,311.0,15.2,396.9,19.15,27.1]  |\n",
      "|0.524|[0.21124,12.5,7.87,0.0,5.631,100.0,6.0821,5.0,311.0,15.2,386.63,29.93,16.5]|\n",
      "|0.524|[0.17004,12.5,7.87,0.0,6.004,85.9,6.5921,5.0,311.0,15.2,386.71,17.1,18.9]  |\n",
      "|0.524|[0.22489,12.5,7.87,0.0,6.377,94.3,6.3467,5.0,311.0,15.2,392.52,20.45,15.0] |\n",
      "|0.524|[0.11747,12.5,7.87,0.0,6.009,82.9,6.2267,5.0,311.0,15.2,396.9,13.27,18.9]  |\n",
      "|0.524|[0.09378,12.5,7.87,0.0,5.889,39.0,5.4509,5.0,311.0,15.2,390.5,15.71,21.7]  |\n",
      "|0.538|[0.62976,0.0,8.14,0.0,5.949,61.8,4.7075,4.0,307.0,21.0,396.9,8.26,20.4]    |\n",
      "|0.538|[0.63796,0.0,8.14,0.0,6.096,84.5,4.4619,4.0,307.0,21.0,380.02,10.26,18.2]  |\n",
      "|0.538|[0.62739,0.0,8.14,0.0,5.834,56.5,4.4986,4.0,307.0,21.0,395.62,8.47,19.9]   |\n",
      "|0.538|[1.05393,0.0,8.14,0.0,5.935,29.3,4.4986,4.0,307.0,21.0,386.85,6.58,23.1]   |\n",
      "|0.538|[0.7842,0.0,8.14,0.0,5.99,81.7,4.2579,4.0,307.0,21.0,386.75,14.67,17.5]    |\n",
      "|0.538|[0.80271,0.0,8.14,0.0,5.456,36.6,3.7965,4.0,307.0,21.0,288.99,11.69,20.2]  |\n",
      "|0.538|[0.7258,0.0,8.14,0.0,5.727,69.5,3.7965,4.0,307.0,21.0,390.95,11.28,18.2]   |\n",
      "+-----+---------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "inputCols = [\n",
    "    'CRIM',\n",
    " 'ZN',\n",
    " 'INDUS',\n",
    " 'CHAS',\n",
    " 'RM',\n",
    " 'AGE',\n",
    " 'DIS',\n",
    " 'RAD',\n",
    " 'TAX',\n",
    " 'PT',\n",
    " 'B',\n",
    " 'LSTAT',\n",
    " 'MV'\n",
    "]\n",
    "outputCol = \"features\"\n",
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "df = df_va.transform(house_df)\n",
    "df = df.select(['nox','features'])\n",
    "newcolumns = ['label','features']\n",
    "df.toDF(*newcolumns).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f63383b-a97b-430a-bf7f-bdd685c2b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membagi data\n",
    "(trainingData, testData) = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecd4ce82-52be-48ed-bd8f-384f7029c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|  nox|            features|\n",
      "+-----+--------------------+\n",
      "|0.392|[0.03548,80.0,3.6...|\n",
      "|0.392|[0.04819,80.0,3.6...|\n",
      "|0.394|[0.01538,90.0,3.7...|\n",
      "|0.398|[0.04379,80.0,3.3...|\n",
      "|  0.4|[0.00906,90.0,2.9...|\n",
      "|  0.4|[0.05561,70.0,2.2...|\n",
      "|0.401|[0.01439,60.0,2.9...|\n",
      "|0.401|[0.01501,90.0,1.2...|\n",
      "|0.401|[0.02187,60.0,2.9...|\n",
      "|0.403|[0.01778,95.0,1.4...|\n",
      "|0.403|[0.0315,95.0,1.47...|\n",
      "|0.404|[0.03768,80.0,1.5...|\n",
      "|0.404|[0.04011,80.0,1.5...|\n",
      "|0.404|[0.04666,80.0,1.5...|\n",
      "|0.405|[0.03871,52.5,5.3...|\n",
      "|0.405|[0.04297,52.5,5.3...|\n",
      "|0.409|[0.05789,12.5,6.0...|\n",
      "| 0.41|[0.0136,75.0,4.0,...|\n",
      "|0.411|[0.03615,80.0,4.9...|\n",
      "|0.411|[0.07244,60.0,1.6...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56ecc48e-b27f-48ee-90fa-e8b95beb678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|   nox|            features|\n",
      "+------+--------------------+\n",
      "| 0.385|[0.01965,80.0,1.7...|\n",
      "| 0.389|[0.01096,55.0,2.2...|\n",
      "| 0.398|[0.03584,80.0,3.3...|\n",
      "|   0.4|[0.04417,70.0,2.2...|\n",
      "|   0.4|[0.06466,70.0,2.2...|\n",
      "| 0.403|[0.01311,90.0,1.2...|\n",
      "| 0.405|[0.0459,52.5,5.32...|\n",
      "| 0.409|[0.12816,12.5,6.0...|\n",
      "| 0.409|[0.13554,12.5,6.0...|\n",
      "|  0.41|[0.01709,90.0,2.0...|\n",
      "|  0.41|[0.02055,85.0,0.7...|\n",
      "| 0.411|[0.01432,100.0,1....|\n",
      "| 0.411|[0.03502,80.0,4.9...|\n",
      "| 0.413|[0.04301,80.0,1.9...|\n",
      "|0.4161|[0.02009,95.0,2.6...|\n",
      "|0.4161|[0.0351,95.0,2.68...|\n",
      "| 0.426|[0.04462,25.0,4.8...|\n",
      "| 0.428|[0.09252,30.0,4.9...|\n",
      "| 0.428|[0.1029,30.0,4.93...|\n",
      "| 0.431|[0.1403,22.0,5.86...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eafbf753-ab39-4360-ba4d-c8c7b01b5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Melatih model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"nox\")\n",
    "model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0dfe350-423c-4017-bf3d-8a253d776abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.000565736235050674,-3.1996235109388945e-05,0.0038575593627519274,-0.0073594935050072165,-0.00786112398153221,0.000809958080738571,-0.018822829856001692,0.0024933259358736323,7.841173656238722e-05,-0.01256680744065956,2.191882928070262e-06,0.0002655770991284851,-0.001395396443715132]\n",
      "Intercept: 0.7832464233929658\n"
     ]
    }
   ],
   "source": [
    "# Melihat koefisien model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "\n",
    "# Melihat intercept model\n",
    "print(\"Intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed7c264f-26b0-47a0-a344-6495a31876fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.regression.LinearRegressionTrainingSummary at 0x7f752e624450>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mendapatkan ringkasan model\n",
    "trainingSummary = model.summary\n",
    "trainingSummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2ddeb91-6e43-4b3a-9402-71533f021a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memprediksi\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae6b1aa6-f2a5-41eb-b2b3-49e87afe6965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+\n",
      "|   nox|            features|         prediction|\n",
      "+------+--------------------+-------------------+\n",
      "| 0.385|[0.01965,80.0,1.7...|0.36172917396547893|\n",
      "| 0.389|[0.01096,55.0,2.2...|0.43382255453093793|\n",
      "| 0.398|[0.03584,80.0,3.3...|0.43758091418055384|\n",
      "|   0.4|[0.04417,70.0,2.2...|0.44907173573724257|\n",
      "|   0.4|[0.06466,70.0,2.2...|0.43395093603157986|\n",
      "| 0.403|[0.01311,90.0,1.2...| 0.3401137928556448|\n",
      "| 0.405|[0.0459,52.5,5.32...| 0.4527201439880943|\n",
      "| 0.409|[0.12816,12.5,6.0...|  0.437897792249772|\n",
      "| 0.409|[0.13554,12.5,6.0...| 0.4492849139810452|\n",
      "|  0.41|[0.01709,90.0,2.0...| 0.3097744152578325|\n",
      "|  0.41|[0.02055,85.0,0.7...| 0.3692308317094424|\n",
      "| 0.411|[0.01432,100.0,1....|0.40825319873249727|\n",
      "| 0.411|[0.03502,80.0,4.9...|0.42200072440697856|\n",
      "| 0.413|[0.04301,80.0,1.9...| 0.2992718307237377|\n",
      "|0.4161|[0.02009,95.0,2.6...|0.43153636710676585|\n",
      "|0.4161|[0.0351,95.0,2.68...|  0.436348653618187|\n",
      "| 0.426|[0.04462,25.0,4.8...|  0.467173151553065|\n",
      "| 0.428|[0.09252,30.0,4.9...|0.46715041352224373|\n",
      "| 0.428|[0.1029,30.0,4.93...| 0.4643772979162462|\n",
      "| 0.431|[0.1403,22.0,5.86...|0.39706691300307234|\n",
      "+------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43c7aa8a-018e-45b3-bb42-62aec7ac901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05801174055311912"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Evaluasi\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\",labelCol=\"nox\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfd04b-7603-42c4-9eef-2e7ec6cf9764",
   "metadata": {},
   "source": [
    "### Decision Tree Classiication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6fc9018-2c20-4b30-87cd-3ee0864156a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "data = df.toDF(*newcolumns)\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7add727-f9bf-4c9b-a50e-144377d49da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4d5079b-d099-41e7-8f22-dd59bdc98688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9d3e3a7-16e6-4dcc-ad1f-5abbcbb35d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a71ad47-e9a5-424b-b1da-ad2a6b692a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cf37933-b7e4-45db-ad80-7f4443bf7d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       2.0|        75.0|[0.01965,80.0,1.7...|\n",
      "|       2.0|        76.0|[0.01096,55.0,2.2...|\n",
      "|       2.0|        77.0|[0.01538,90.0,3.7...|\n",
      "|       2.0|        68.0|[0.03584,80.0,3.3...|\n",
      "|       2.0|        43.0|[0.04417,70.0,2.2...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c47a0f1-4917-4430-94fe-3bd1c9cc1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acccuracy = 0.295302 \n",
      "Test Error = 0.704698 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Acccuracy = %g \" % accuracy)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14a72596-95df-41aa-baf8-c9371a045076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_251a26391ccb, depth=5, numNodes=27, numClasses=81, numFeatures=13\n"
     ]
    }
   ],
   "source": [
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2a34d-ca9e-4a62-b3b4-8360ba7e95a8",
   "metadata": {},
   "source": [
    "### K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87408bd8-fe89-4e2a-a41d-7cfb554ef0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67e4b774-f4fe-44b3-8f6b-21173cc53bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "|label|features                 |\n",
      "+-----+-------------------------+\n",
      "|0.0  |(3,[],[])                |\n",
      "|1.0  |(3,[0,1,2],[0.1,0.1,0.1])|\n",
      "|2.0  |(3,[0,1,2],[0.2,0.2,0.2])|\n",
      "|3.0  |(3,[0,1,2],[9.0,9.0,9.0])|\n",
      "|4.0  |(3,[0,1,2],[9.1,9.1,9.1])|\n",
      "|5.0  |(3,[0,1,2],[9.2,9.2,9.2])|\n",
      "+-----+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"sample_kmeans_data.txt\")\n",
    "dataset.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "217f3720-b6a5-447f-981a-b5bf1907fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3844eee6-b660-4893-8639-312fc33d0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "191c0bcd-758d-4860-881b-673cb820ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1747302f-71a5-4b66-85d4-c1c626dbe68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
